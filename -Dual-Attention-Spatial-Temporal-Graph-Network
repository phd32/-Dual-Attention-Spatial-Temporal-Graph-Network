import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class RelevanceGating(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.Wr = nn.Linear(hidden_dim, hidden_dim)
        self.u = nn.Parameter(torch.randn(hidden_dim))
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.xavier_uniform_(self.Wr.weight)
        nn.init.zeros_(self.Wr.bias)
        nn.init.normal_(self.u)
    
    def forward(self, hi, hj):
        # Calculate relevance gate
        r_input = torch.tanh(self.Wr(hi) + self.Wr(hj))
        R_ij = torch.sigmoid((r_input @ self.u))
        return R_ij

class SpatialGraphAttention(nn.Module):
    def __init__(self, in_features, out_features, heads=8):
        super().__init__()
        self.heads = heads
        self.out_features = out_features
        self.head_dim = out_features // heads
        
        self.Q = nn.Linear(in_features, out_features)
        self.K = nn.Linear(in_features, out_features)
        self.V = nn.Linear(in_features, out_features)
        self.W = nn.Linear(in_features, out_features)
        
        self.relevance_gating = RelevanceGating(in_features)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.xavier_uniform_(self.Q.weight)
        nn.init.xavier_uniform_(self.K.weight)
        nn.init.xavier_uniform_(self.V.weight)
        nn.init.xavier_uniform_(self.W.weight)
    
    def forward(self, h, adj_matrix):
        batch_size, num_nodes, _ = h.shape
        
        # Local attention with relevance gating
        local_attention_scores = torch.zeros(batch_size, num_nodes, num_nodes, device=h.device)
        
        for i in range(num_nodes):
            neighbors = torch.nonzero(adj_matrix[i], as_tuple=True)[0]
            for j in neighbors:
                R_ij = self.relevance_gating(h[:, i], h[:, j])
                # Simple attention score (can be enhanced)
                alpha_ij = torch.sum(h[:, i] * h[:, j], dim=-1, keepdim=True)
                local_attention_scores[:, i, j] = R_ij.squeeze() * alpha_ij.squeeze()
        
        # Global attention (transformer-style)
        Q = self.Q(h).view(batch_size, num_nodes, self.heads, self.head_dim)
        K = self.K(h).view(batch_size, num_nodes, self.heads, self.head_dim)
        V = self.V(h).view(batch_size, num_nodes, self.heads, self.head_dim)
        
        attention_scores = torch.einsum('bihd,bjhd->bijh', Q, K) / math.sqrt(self.head_dim)
        global_attention = F.softmax(attention_scores, dim=2)
        
        # Combine local and global
        gamma = nn.Parameter(torch.tensor(0.5))
        combined_attention = gamma * local_attention_scores.unsqueeze(-1) + (1 - gamma) * global_attention
        
        # Apply attention
        out = torch.einsum('bijh,bjhd->bihd', combined_attention, V)
        out = out.contiguous().view(batch_size, num_nodes, self.out_features)
        
        return out

class TemporalTransformer(nn.Module):
    def __init__(self, hidden_dim, num_heads, num_layers):
        super().__init__()
        self.time_embedding = nn.Linear(1, hidden_dim)
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)
            for _ in range(num_layers)
        ])
    
    def forward(self, x, time_intervals):
        # Add time embeddings
        time_emb = self.time_embedding(time_intervals.unsqueeze(-1))
        x = x + time_emb
        
        for layer in self.layers:
            x = layer(x)
        
        return x

class DualAttentionSpatioTemporalGraph(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, num_heads=8, num_temporal_layers=2):
        super().__init__()
        
        # CNN Encoder for ROIs
        self.cnn_encoder = nn.Sequential(
            nn.Conv3d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(),
            nn.Conv3d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm3d(64),
            nn.ReLU(),
            nn.Conv3d(64, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm3d(hidden_dim),
            nn.ReLU(),
            nn.AdaptiveAvgPool3d(1)
        )
        
        self.spatial_attention = SpatialGraphAttention(hidden_dim, hidden_dim, num_heads)
        self.temporal_transformer = TemporalTransformer(hidden_dim, num_heads, num_temporal_layers)
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_classes)
        )
    
    def forward(self, x, adj_matrix, time_intervals):
        # x: (batch, channels, depth, height, width)
        batch_size = x.shape[0]
        
        # Extract features from CNN
        features = self.cnn_encoder(x).squeeze(-1).squeeze(-1).squeeze(-1)  # (batch, hidden_dim)
        
        # Spatial attention
        spatial_features = self.spatial_attention(features.unsqueeze(1), adj_matrix)
        
        # Temporal transformer
        temporal_features = self.temporal_transformer(spatial_features, time_intervals)
        
        # Global pooling and classification
        pooled = torch.mean(temporal_features, dim=1)
        output = self.classifier(pooled)
        
        return output
